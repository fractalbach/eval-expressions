package lexer

import (
	"fmt"
	"unicode"
	"github.com/fractalbach/eval-expressions/token"
)

// ======================================================================
// The Lexer : How the user interacts with this program
// ======================================================================

// Lexer does lexical anlaysis of an input string.  It will tokenize
// an input string, save the generated tokens, and/or leave an error if there
// is a syntax error.
type Lexer struct {
	tokens []token.Token
	err    error
}

// New creates a new Lexer structure.
func New() *Lexer {
	return &Lexer{
		tokens: []token.Token{},
		err:    nil,
	}
}

// Tokenize takes an input string and saves the tokens generated, or
// produces an error.
func (l *Lexer) Tokenize(s string) {
	l.clear()
	l.tokenize(s)
}

// Error returns nil if there is no error, or a message that indicates
// there was a problem during tokenization.
func (l *Lexer) Error() error {
	return l.err
}

// Display will print all of the tokens generated by the Lexer from
// the most recent tokenization
func (l *Lexer) Display() {
	for _, v := range l.tokens {
		fmt.Println(v)
	}
}

// Tokens returns a list of tokens that can be sent to the parser.
func (l *Lexer) Tokens() []token.Token {
	return l.tokens
}

func (l *Lexer) clear() {
	l.tokens = []token.Token{}
	l.err = nil
}

// ======================================================================
// Tokenization Process
// ======================================================================

var (
	content        = ""
	kind           = ""
	insideToken    = false
	dotUsedAlready = false
)

func (l *Lexer) tokenize(s string) {
	reset()
	for _, r := range s {
		switch r {

		case '(', ')', '+', '-', '*', '/', '=':
			l.pushLastToken()
			l.push(token.SymbolToken, string(r))
			continue

		case '.':
			if kind == token.NoKind {
				l.setErr("cannot use \".\" to start a number")
				return
			}
			if kind != token.NumberToken {
				l.setErr("cannot use \".\" outside number")
				return
			}
			if dotUsedAlready {
				l.setErr("too many \".\"s in a number")
				return
			}
			dotUsedAlready = true
			content += "."
			continue
		}
		switch {

		case unicode.IsSpace(r):
			continue

		case unicode.IsDigit(r):
			if !insideToken {
				startToken(token.NumberToken)
			}
			content += string(r)
			continue

		case unicode.IsLetter(r):
			if !insideToken {
				startToken(token.IDToken)
			}
			if kind == token.NumberToken {
				l.setErr("cannot start identifier with a number.")
				return
			}
			content += string(r)
			continue
		}
		l.setErr(`Invalid symbol "` + string(r) + `"`)
		return
	}
	l.pushLastToken()
}

func (l *Lexer) setErr(i ...interface{}) {
	msg := ""
	msg += "__________ Syntax Error __________\n"
	msg += "That's a mean expression to give me =(\n"
	msg += fmt.Sprint(i...)
	l.err = fmt.Errorf("%s", msg)
}

func (l *Lexer) pushLastToken() {
	if insideToken {
		l.push(kind, content)
	}
	reset()
}

func (l *Lexer) push(kind, content string) {
	l.tokens = append(l.tokens, token.New(kind, content))
}


func startToken(name string) {
	insideToken = true
	kind = name
}

func reset() {
	insideToken = false
	kind = ""
	content = ""
	dotUsedAlready = false
}
