package lexer

import (
	"fmt"
	"github.com/fractalbach/eval-expressions/token"
	"unicode"
)

// ======================================================================
// The Lexer : How the user interacts with this program
// ======================================================================

// Lexer does lexical anlaysis of an input string.  It will tokenize
// an input string, save the generated tokens, and/or leave an error if there
// is a syntax error.
type Lexer struct {
	tokens []token.Token
	err    error
}

// New creates a new Lexer structure.
func New() *Lexer {
	return &Lexer{
		tokens: []token.Token{},
		err:    nil,
	}
}

// Tokenize takes an input string and saves the tokens generated, or
// produces an error.
func (l *Lexer) Tokenize(s string) {
	l.clear()
	l.tokenize(s)
}

// Error returns nil if there is no error, or a message that indicates
// there was a problem during tokenization.
func (l *Lexer) Error() error {
	return l.err
}

// Display will print all of the tokens generated by the Lexer from
// the most recent tokenization
func (l *Lexer) Display() {
	for _, v := range l.tokens {
		fmt.Println(v)
	}
}

// Tokens returns a list of tokens that can be sent to the parser.
func (l *Lexer) Tokens() []token.Token {
	return l.tokens
}

func (l *Lexer) clear() {
	l.tokens = []token.Token{}
	l.err = nil
}

// ======================================================================
// Fun Error Messages
// ======================================================================

const errNumWord = "starting words with numbers is just confusing o_o !"

// ======================================================================
// Tokenization Process
// ======================================================================

var (
	content        = ""
	insideToken    = false
	insideNum      = false
	insideWord     = false
	dotUsedAlready = false
)

func (l *Lexer) tokenize(s string) {
	reset()
	for _, r := range s {
		switch r {
			
		case '(', ')', '+', '-', '*', '/', '=':
			l.pushLastToken()
			tok := token.NewFromSymbol(r)
			l.push(tok)
			continue

		case '.':
			if !insideToken {
				insideToken = true
				insideNum = true
				content += "0."
				return
			}
			if !insideNum {
				l.setErr("cannot use \".\" outside number")
				return
			}
			if dotUsedAlready {
				l.setErr("too many \".\"s in a number")
				return
			}
			dotUsedAlready = true
			content += "."
			continue
		}
		switch {
			
		case unicode.IsSpace(r):
			continue

		case unicode.IsDigit(r):
			if !insideToken {
				insideToken = true
				insideNum = true
			}
			content += string(r)
			continue

		case unicode.IsLetter(r):
			if !insideToken {
				insideToken = true
				insideWord = true
			}
			if insideNum {
				l.setErr(errNumWord)
				return
			}
			content += string(r)
			continue
		}
		l.setErr(`Invalid symbol "` + string(r) + `"`)
		return
	}
	l.pushLastToken()
}

func (l *Lexer) setErr(i ...interface{}) {
	msg := ""
	msg += "__________ Syntax Error __________\n"
	msg += "If you're gonna be like that,\n"
	msg += "then I'm not gonna answer you =P\n"
	msg += "ERROR: "
	msg += fmt.Sprint(i...)
	l.err = fmt.Errorf("%s", msg)
}

func (l *Lexer) pushLastToken() {
	if !insideToken {
		return
	}
	if insideNum {
		l.push(token.NewNum(content))
	}
	if insideWord {
		l.push(token.NewWord(content))
	}
	reset()
}

func (l *Lexer) push(tok token.Token) {
	l.tokens = append(l.tokens, tok)
}

func reset() {
	insideToken = false
	insideNum = false
	insideWord = false
	dotUsedAlready = false
	content = ""
}
