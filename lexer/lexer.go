package lexer

import (
	"fmt"
	"unicode"
)

// ======================================================================
// The Lexer : How the user interacts with this program
// ======================================================================

// Lexer does lexical anlaysis of an input string.  It will tokenize
// an input string, save the generated tokens, and/or leave an error if there
// is a syntax error.
type Lexer struct {
	tokens []token
	err    error
}

// New creates a new Lexer structure.
func New() *Lexer {
	return &Lexer{
		tokens: []token{},
		err:    nil,
	}
}

// Tokenize takes an input string and saves the tokens generated, or
// produces an error.
func (l *Lexer) Tokenize(s string) {
	l.clear()
	l.tokenize(s)
}

// Error returns nil if there is no error, or a message that indicates
// there was a problem during tokenization.
func (l *Lexer) Error() error {
	return l.err
}

// Display will print all of the tokens generated by the Lexer from
// the most recent tokenization
func (l *Lexer) Display() {
	for _, v := range l.tokens {
		fmt.Println(v)
	}
}

func (l *Lexer) clear() {
	l.tokens = []token{}
	l.err = nil
}

// ======================================================================
// Token Structure
// ======================================================================

type token struct {
	kind    string
	content string
}

func (t token) String() string {
	return fmt.Sprintf("<%s> %s </%s>", t.kind, t.content, t.kind)
}

// ======================================================================
// Tokenization Process
// ======================================================================

const (
	noKind = ""
	number = "number"
	word = "word"
)

var (
	content        = ""
	kind           = ""
	insideToken    = false
	dotUsedAlready = false
)

func (l *Lexer) tokenize(s string) {
	reset()
	for _, r := range s {
		switch r {
		case '(', ')', '+', '-', '*', '/', '=':
			l.pushLastToken()
			l.push("symbol", string(r))
			continue

		case '.':
			if kind == noKind {
				l.setErr("cannot use \".\" to start a number")
				return
			}
			if kind != number {
				l.setErr("cannot use \".\" outside number")
				return
			}
			if dotUsedAlready {
				l.setErr("too many \".\"s in a number")
				return
			}
			dotUsedAlready = true
			content += "."
			continue
		}
		switch {
		case unicode.IsSpace(r):
			continue

		case unicode.IsDigit(r):
			if !insideToken {
				startToken("number")
			}
			content += string(r)
			continue

		case unicode.IsLetter(r):
			if !insideToken {
				startToken("word")
			}
			if kind == number {
				l.setErr("cannot start identifier with a number.")
				return
			}
			content += string(r)
			continue
		}
		l.setErr(`Invalid symbol "` + string(r) + `"`)
		return
	}
	l.pushLastToken()
}

func (l *Lexer) setErr(i ...interface{}) {
	msg := ""
	msg += "__________ Syntax Error __________\n"
	msg += "That's a mean expression to give me =(\n"
	msg += fmt.Sprint(i...)
	l.err = fmt.Errorf("%s", msg)
}

func (l *Lexer) pushLastToken() {
	if insideToken {
		l.push(kind, content)
	}
	reset()
}

func (l *Lexer) push(kind, content string) {
	l.tokens = append(l.tokens, token{
		kind:    kind,
		content: content,
	})
}

func startToken(name string) {
	insideToken = true
	kind = name
}

func reset() {
	insideToken = false
	kind = ""
	content = ""
	dotUsedAlready = false
}
